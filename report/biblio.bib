@misc{adam,
Author = {Diederik P. Kingma and Jimmy Ba},
Title = {Adam: A Method for Stochastic Optimization},
Year = {2014},
Eprint = {arXiv:1412.6980},
}

@misc{adagrad,
Author = {Rachel Ward and Xiaoxia Wu and Leon Bottou},
Title = {AdaGrad stepsizes: Sharp convergence over nonconvex landscapes, from any initialization},
Year = {2018},
Eprint = {arXiv:1806.01811},
}

@inproceedings{zinka,
author = {Zinkevich, Martin},
title = {Online Convex Programming and Generalized Infinitesimal Gradient Ascent},
year = {2003},
isbn = {1577351894},
publisher = {AAAI Press},
booktitle = {Proceedings of the Twentieth International Conference on International Conference on Machine Learning},
pages = {928–935},
numpages = {8},
location = {Washington, DC, USA},
series = {ICML’03}
}

@misc{rmsprop,
Author = {Sebastian Ruder},
Title = {An overview of gradient descent optimization algorithms},
Year = {2016},
Eprint = {arXiv:1609.04747},
}
@misc{adadelta,
Author = {Matthew D. Zeiler},
Title = {ADADELTA: An Adaptive Learning Rate Method},
Year = {2012},
Eprint = {arXiv:1212.5701},
}


@inproceedings{limits,
Author = {Liangchen Lua and al},
Title = {ADAPTIVE GRADIENT METHODS WITH DYNAMIC BOUND OF LEARNING RATE},
Howpublished={\url{https://openreview.net/pdf?id=Bkg3g2R9FX}},
Year=2019
}

@misc{adamtrend,
Author = {Vitaly Bushaev},
Title = {Adam — latest trends in deep learning optimization},
Year = {2018},
Howpublished = \url{https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c}
}
